{
  "papers": [
    {
      "id": 1,
      "shortTitle": "Paper One",
      "title": "Product Health Insights Using Telemetry",
      "author": "Chris",
      "algorithm": "Logistic Regression and Significance Testing",
      "thumbnail": "paper-chris-thumbnail.png",
      "analysis": [
        {
          "type": "text",
          "content": "This paper sought to assess whether a certain feature was significantly present on the same day that an uncorrected error occurred (think blue screen of death). There are many different types of uncorrected errors, so they looked at the top 30. They looked at two different features, daily max temperature and presence of a corrected error (an error that the OS manages to resolve). For each of these features, they made a univariate logistic regression model (two total times 30 uncorrected error types = 60 total models) to predict whether an uncorrected error occurred on a given day and with that coefficient they took a statistical test to assess whether or not that variable was statistically significant with an alpha = 0.05."
        }
      ],
      "privatization": [
        {
          "type": "text",
          "content": "We looked at how corrected errors predict uncorrected errors and dropped daily max temperature. We trained on the top thirty uncorrected errors as well, and dropped one due to large compute times, for a total of 29 different logistic regression models. Privacy was applied in the form of differentially private gradient descent, which means that for each step of gradient descent, a small amount of noise is added. This does mean that compute scales linearly with epsilon, as higher epsilons means more noisy gradient descent steps, thus we stopped at epsilon 1.5 for each of the models."
        },
        {
          "type": "text",
          "content": " In order to ensure that our alpha would have the same statistical power as the non-private version, we did a permutation test for each of the logistic regression models to empirically find a p-value for the private logistic regression models. Because of the long compute times with epsilon, we took 200 permutations for each private logistic regression model to compare our result against."
        }
      ],
      "results": [
        {
          "type": "text",
          "content": "Our main method of comparing against the baseline was by calculating the set intersection over union for the private models and the non-private models. Firstly, for a \"strong DP\" (epsilon = 1), the model severely under-shot the number of significant models. This means that it failed to find the correct relationship between corrected and uncorrected errors in a third (0.31) of the models."
        },
        {
          "type": "image",
          "src": "lr_pval_1.png",
          "alt": "Confusion Matrix",
          "caption": "Figure 1:  A diagram of the confusion matrix. Notice zero false positives and nine false negatives."
        },
        {
          "type": "text",
          "content": "Overall, as the epsilon increased from zero to 0.5, the intersection over union tended to increase. However, between 0.5 and 1.5, the intersection over union failed to approach the non-private model and failed to improve."
        }
      ],
      "interpretation": [
        {
          "type": "text",
          "content": "Due to high compute costs, we were only able to get up to epsilon 1.5 for each of the 29 models. As we expected, the utility would increase from very low epsilon but around ε = 0.5, it is unclear whether or not the utility would continue to increase. At ε = 1, the model does somewhat poorly as seen in Figure 1. The model has an intersect over union of around 0.60 and notably identifies a majority of the models as not significant, a result contrary to the nonprivate model. Overall, this analysis task did not seem to be replicable privately, at least under the strict privacy constraints."
        },
        {
          "type": "text",
          "content": "It is important to note that this analysis would not be nearly as computationally hungry if we weren't trying to compare the models to a non-private model. In the practical setting, we wouldn't need to do permutation testing to find our p-values empirically. Similarly to the non-private setting, our alpha would be a hyperparameter that we would be able to select ourselves. This means that we may be able to use much higher values of epsilon in practice"
        }
      ]
    },
    {
      "id": 2,
      "shortTitle": "Paper Two",
      "title": "Power Consumption Patterns in Intel's Telemetry Data",
      "author": "Trey",
      "algorithm": "LASSO Regression",
      "thumbnail": "paper-trey-thumbnail.png",
      "analysis": [
        {
          "type": "text",
          "content": "This paper sought to assess whether a certain feature was significantly present on the same day that an uncorrected error occurred (think blue screen of death). There are many different types of uncorrected errors, so they looked at the top 30. They looked at two different features, daily max temperature and presence of a corrected error (an error that the OS manages to resolve). For each of these features, they made a univariate logistic regression model (two total times 30 uncorrected error types = 60 total models) to predict whether an uncorrected error occurred on a given day and with that coefficient they took a statistical test to assess whether or not that variable was statistically significant with an alpha = 0.05."
        }
      ],
      "privatization": [
        {
          "type": "text",
          "content": "We looked at how corrected errors predict uncorrected errors and dropped daily max temperature. We trained on the top thirty uncorrected errors as well, and dropped one due to large compute times, for a total of 29 different logistic regression models. Privacy was applied in the form of differentially private gradient descent, which means that for each step of gradient descent, a small amount of noise is added. This does mean that compute scales linearly with epsilon, as higher epsilons means more noisy gradient descent steps, thus we stopped at epsilon 1.5 for each of the models. In order to ensure that our alpha would have the same statistical power as the non-private version, we did a permutation test for each of the logistic regression models to empirically find a p-value for the private logistic regression models. Because of the long compute times with epsilon, we took 200 permutations for each private logistic regression model to compare our result against."
        }
      ],
      "results": [
        {
          "type": "text",
          "content": "Our main method of comparing against the baseline was by calculating the set intersection over union for the private models and the non-private models. Firstly, for a \"strong DP\" (epsilon = 1), the model severely under-shot the number of significant models. This means that it failed to find the correct relationship between corrected and uncorrected errors in a third (0.31) of the models."
        },
        {
          "type": "image",
          "src": "lr_pval_1.png",
          "alt": "Confusion Matrix",
          "caption": "Figure 1:  A diagram of the confusion matrix. Notice zero false positives and nine false negatives."
        },
        {
          "type": "text",
          "content": "Overall, as the epsilon increased from zero to 0.5, the intersection over union tended to increase. However, between 0.5 and 1.5, the intersection over union failed to approach the non-private model and failed to improve."
        }
      ],
      "interpretation": [
        {
          "type": "text",
          "content": "Insert interpretation here"
        }
      ]
    },
    {
      "id": 3,
      "shortTitle": "Paper Three",
      "title": "PC Health Impact White Paper",
      "author": "Bradley",
      "algorithm": "K-Means Clustering and Lloyd's Algorithm",
      "thumbnail": "paper-bradley-thumbnail.png",
      "analysis": [
        {
          "type": "text",
          "content": "This paper introduces two tools for detecting and analyzing changes in device usage patterns over time. Tool 1 identifies individual devices that exhibit significant behavioral changes in response to specific events by analyzing task switch data and applying statistical tests. Tool 2 groups devices based on their computer usage patterns over time using clustering techniques, allowing for the identification of different groups with similar behavioral changes. By leveraging large-scale telemetry data on user-task events, these methods help uncover shifts in human-device interactions for time-sensitive events."
        }
      ],
      "privatization": [
        {
          "type": "text",
          "content": "We trained on a subset of data from the large telemetry dataset the paper used due to computing limitations.  Each user is grouped by the number of respective task switches for each week. The week-to-week differences are computed to quantify the total change for each user. This number is then normalized and scaled to between -10 and 10. To prevent any single device from having too much influence, the data is first \"clipped\" to a fixed range (i.e. -9.9 to 9.9), meaning extreme values are capped before processing."
        },
        {
          "type": "text",
          "content": "The algorithm we privatized was Lloyd’s algorithm for k-means clustering. Noise, drawn from a Laplace distribution, is then added to the mean computation in each iteration, ensuring privacy while still allowing meaningful patterns to emerge. After at most 10 iterations, the privatized k-means model is trained and can then be tested on the test set. This process is repeated for epsilon between .01 and 100 which is shown in the next section."
        }
      ],
      "results": [
        {
          "type": "text",
          "content": "The baseline method the paper used was non-private k-means clustering. This had a loss of 1.69 in our testing."
        },
        {
          "type": "image",
          "src": "kmeans_1.png",
          "alt": "K-Means Clustering Loss vs Epsilon",
          "caption": "Figure 1:  A plot showcasing inertia as epsilon increases. Notice where it converges."
        },
        {
          "type": "text",
          "content": "As epsilon increases, the inertia for the privatized model slowly converges to the non-private train and test loss. This is expected and given its convergence close to an epsilon of 1, the model performs quite well in terms of both privacy and accuracy."
        }
      ],
      "interpretation": [
        {
          "type": "text",
          "content": "The clustering algorithm is unsupervised so the best way to quantify performance is using inertia. It is the squared differences between data points and their nearest centroid. The smaller this value is, the better performing the cluster locations are, and therefore the model. "
        },
        {
          "type": "text",
          "content": "The cluster locations are located where the most data points are. Given that the distribution of the data is right skewed, it makes sense that the centroids, or cluster locations, are also right skewed to follow the data."
        },
        {
          "type": "text",
          "content": "From Figure 1, we can see that as epsilon increases, the inertia for the privatized model slowly converges to the non-private train and test loss. This is expected, and given its convergence close to an epsilon of 1, the model performs quite well in terms of both privacy and accuracy."
        }
      ]
    },
    {
      "id": 4,
      "shortTitle": "Paper Four",
      "title": "Exploration of CPU Error Dependencies and Prediction",
      "author": "Tyler",
      "algorithm": "Conditional Probabilities",
      "thumbnail": "paper-tyler-thumbnail.png",
      "analysis": [
        {
          "type": "text",
          "content": "In the paper, the authors first separate each GUID (user) into the number of corrected errors observed during a set time period. They then created a histogram where the x-axis was the number of corrected errors observed and the y-axis was the percentage of GUID’s that observed an uncorrected error."
        }
      ],
      "privatization": [
        {
          "type": "text",
          "content": "We are releasing a percentage for each bin in the histogram, and in order to guarantee privacy, we must add noise to both the numerator and denominator where the numerator is the number of GUID’s that contained an uncorrected error (number of 1’s) and the denominator is the number of GUID’s total.  The noise we added was drawn from the Laplace distribution. The sensitivity of the function is defined as the maximum possible absolute change in the output of the function due to the change in a single user’s data. Since we are dealing with a percentage (and we are considering the worst case), this change can be at most 1 user, which corresponds to a sensitivity of 1."
        }
      ],
      "results": [
        {
          "type": "text",
          "content": "We observed a tradeoff between privacy and utility. We observed a tradeoff between privacy and utility. Our results show a significant increase in utility as ε increases. For instance, at ε = 0.1, the utility was approximately 0.65, which rose to nearly 0.99 at ε = 1 and reached almost 1 at ε = 100. This demonstrates that less stringent privacy guarantees (higher ε values) lead to higher utility."
        },
        {
          "type": "image",
          "src": "c_prob_1.png",
          "alt": "MAE vs Epsilon",
          "caption": "Figure 1:  A plot showcasing MAE as epsilon increases. As epsilon increases, the MAE decreases, indicating that the model is more accurate at predicting the number of GUIDs that will have an uncorrected error."
        },
        {
          "type": "image",
          "src": "c_prob_2.png",
          "alt": "Noisy vs Original Histogram",
          "caption": "Figure 2:  A plot showcasing the noisy vs original histogram. Most bins fit fairly well."
        },
        {
          "type": "text",
          "content": "For each epsilon, we found that as the number of corrected errors increased, the utility decreased. This is because the distribution of number of corrected errors is very heavily skewed left. The error amount with the largest amount of unique GUIDs is 0. This trend continues; as the number of corrected errors increases, the number of GUIDs with that exact amount of corrected errors decreases. This means the utility of the function is bounded by the maximum number of errors included in the histogram. We bounded our largest bin to be at 30 and found that an epsilon of 1 gave a sufficient utility. If we wanted to use a stricter epsilon, we would need to reduce the maximum corrected error to maintain the same utility."
        }
      ],
      "interpretation": [
        {
          "type": "text",
          "content": "Similar to established differential privacy theory and literature, we observed a tradeoff between privacy and utility. However, our focus on conditional probability tasks allowed us to explore this tradeoff in a more specific context, providing insights into how differential privacy impacts analytical accuracy in this domain."
        },
        {
          "type": "text",
          "content": "Our results show a significant increase in utility as ε increases. For instance, at ε = 0.1, the utility was approximately 0.65, which rose to nearly 0.99 at ε = 1 and reached almost 1 at ε = 100. This demonstrates that less stringent privacy guarantees (higher ε values) lead to higher utility."
        },
        {
          "type": "text",
          "content": "For each epsilon, we found that as the number of corrected errors increased, the utility decreased due to more noise being added. "
        }
      ]
    }
  ]
}
