{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sample LASSO task functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# set path for jupyter notebook\n",
    "if os.path.exists(os.path.abspath('..')):\n",
    "    sys.path.append(os.path.abspath('..'))\n",
    "\n",
    "from src.feat_build import main\n",
    "from src.model_build import train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inv_dir = Path(os.getcwd())\n",
    "proj_dir = inv_dir.parent\n",
    "\n",
    "sample_guids_parquet = 'sample_guid_10000_china_us.parquet'\n",
    "directories = [\"frgnd_backgrnd_apps_v4_hist\", \"web_cat_usage_v2\",\"power_acdc_usage_v4_hist\",\"os_c_state\", \"hw_pack_run_avg_pwr\"]\n",
    "\n",
    "if 'feat.parquet' not in os.listdir(inv_dir / 'out'):\n",
    "    main.generate_features(sample_guids_parquet, inv_dir, directories)\n",
    "else:\n",
    "    print('Features already generated')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generates Synthetic Data\n",
    "# main.generate_synthetic_data(proj_dir / 'dummy_data')\n",
    "# syn_feat = pd.read_parquet(proj_dir / \"dummy_data\" / \"synthetic_data.parquet\")\n",
    "# syn_feat.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read in Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat = pd.read_parquet(os.path.join('out', 'feat.parquet'))\n",
    "feat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feat['cpu_suffix_Core-U'].value_counts()\n",
    "\n",
    "# we are only interested in U series CPU's\n",
    "feat = feat.loc[feat['cpu_suffix_Core-U']==1,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sklearn lasso alpha: 0.01 on research data for testing\n",
    "best_feats = ['sw_category_Development & Programming (IDEs, Text Editors, Version Control)',\n",
    "       'sw_category_Gaming (Casual, Online & Offline)',\n",
    "       'sw_category_Multimedia Editing (Audio & Video)', 'sw_category_Other',\n",
    "       'sw_category_Simulation & Virtual Reality',\n",
    "       'sw_category_System & Utilities',\n",
    "       'sw_category_Web Browsers & Communication',\n",
    "       'sw_event_name_DC_DISPLAY_OFF', 'sw_event_name_DC_DISPLAY_ON',\n",
    "       'temp_avg', 'web_parent_category_content creation',\n",
    "       'web_parent_category_education', 'web_parent_category_entertainment',\n",
    "       'web_parent_category_games', 'web_parent_category_news',\n",
    "       'web_parent_category_private', 'web_parent_category_reference',\n",
    "       'web_sub_category_communication',\n",
    "       'web_sub_category_music / audio streaming', 'web_sub_category_news',\n",
    "       'web_sub_category_presentations', 'web_sub_category_reference',\n",
    "       'web_sub_category_spreadsheets', 'web_sub_category_video games',\n",
    "       'web_sub_category_word processing', 'cpu_norm_usage', 'nrs',\n",
    "       'countryname_normalized_China', 'modelvendor_normalized_Apple',\n",
    "       'modelvendor_normalized_Dell', 'modelvendor_normalized_LG',\n",
    "       'modelvendor_normalized_Lenovo', 'modelvendor_normalized_Other',\n",
    "       'modelvendor_normalized_Razer', 'modelvendor_normalized_Timi',\n",
    "       'os_Win10', 'os_Win11', 'graphicsmanuf_AMD', 'graphicsmanuf_Intel',\n",
    "       'graphicsmanuf_Nvidia', 'graphicsmanuf_Other', 'cpu_family_Core i3',\n",
    "       'cpu_family_Core i5', 'cpu_suffix_Core-H', 'cpu_suffix_Core-U',\n",
    "       'persona_Casual User', 'persona_Communication', 'persona_Gamer',\n",
    "       'persona_Office/Productivity', 'persona_Win Store App User',\n",
    "       'age_category', '#ofcores', 'screensize_category', 'day_of_week',\n",
    "       'month_of_year']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.linear_model import LassoCV\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "# from sklearn.metrics import mean_squared_error\n",
    "# from sklearn.pipeline import make_pipeline\n",
    "\n",
    "# # Assuming you have your data in X and y\n",
    "# X, y = feat.drop(\"power_mean\", axis=1), feat[\"power_mean\"]\n",
    "\n",
    "# # Split the data into training and test sets\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# # Create a pipeline with StandardScaler and LassoCV\n",
    "# lasso_pipeline = make_pipeline(\n",
    "#     #StandardScaler(),\n",
    "#     LassoCV(cv=20, alphas = np.logspace(-2, 0, 50), random_state=42, max_iter=10000)\n",
    "# )\n",
    "\n",
    "# # Fit the model\n",
    "# lasso_pipeline.fit(X_train, y_train)\n",
    "\n",
    "# # Make predictions\n",
    "# y_train_pred = lasso_pipeline.predict(X_train)\n",
    "# y_test_pred = lasso_pipeline.predict(X_test)\n",
    "\n",
    "# # Calculate MSE for training and test sets\n",
    "# train_mse = mean_squared_error(y_train, y_train_pred)\n",
    "# test_mse = mean_squared_error(y_test, y_test_pred)\n",
    "\n",
    "# print(f\"Training MSE: {train_mse:.4f}\")\n",
    "# print(f\"Test MSE: {test_mse:.4f}\")\n",
    "\n",
    "# # Get the best alpha value\n",
    "# best_alpha = lasso_pipeline.named_steps['lassocv'].alpha_\n",
    "# print(f\"Best alpha: {best_alpha:.4f}\")\n",
    "\n",
    "# # Get the number of features selected\n",
    "# n_features = np.sum(lasso_pipeline.named_steps['lassocv'].coef_ != 0)\n",
    "# print(f\"Number of features selected: {n_features}\")\n",
    "# best_feats = lasso_pipeline.named_steps['lassocv'].coef_\n",
    "# best_feats = X.columns[[i for i in range(len(best_feats)) if best_feats[i] != 0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trivial Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trivial_mse, trivial_model, trivial_r2, trivial_sim = train.trivial(feat, best_feats)\n",
    "trivial_mse, trivial_r2, trivial_sim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run baseline non-private case with no regularization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_mse, baseline_feat_dict, baseline_r2, baseline_sim = train.train(feat, best_feats,\"lasso\", tol=1e-4, l=0)\n",
    "baseline_mse, baseline_r2, baseline_sim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare multiple methods with varying regularization: $l$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare methods\n",
    "results = pd.DataFrame(columns=[\"coord_desc\", \"fw_lap\", \"fw_exp\"])\n",
    "for l in [0.25, 0.5, .9, 1, 5, 10, 25, 100]:\n",
    "    # print(\"parameter: \", l)\n",
    "    test_mse1, feat_dict, r2, similarity = train.train(feat, best_feats, \"lasso\", tol=1e-4, l=1/l) \n",
    "    test_mse2, feat_dict, r2, similarity = train.train(feat, best_feats, \"fw-lasso-lap\", tol=1e-4, l=l, max_iter=500)\n",
    "    test_mse3, feat_dict, r2, similarity = train.train(feat, best_feats, \"fw-lasso-exp\", tol=1e-4, l=l, max_iter=500, normalize=True)\n",
    "    results.loc[l] = [test_mse1, test_mse2, test_mse3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.plot(kind='line', logx=True, marker='.', ylim=(0, 25));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test many epsilon values at regularization $l=10$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epss = [0.25, 0.5, 1, 5, 10, 100, 10_000]\n",
    "epsresults = pd.DataFrame(columns=[\"fw_lap\", \"fw_exp\"])\n",
    "for eps in epss:\n",
    "    print(\"parameter: \", eps)\n",
    "    test_mse1, feat_dict, r2, similarity = train.train(feat, best_feats,\"fw-lasso-lap\", tol=1e-8, l=5, epsilon=eps, max_iter=500)\n",
    "    test_mse2, feat_dict, r2, similarity = train.train(feat, best_feats, \"fw-lasso-exp\", tol=1e-8, l=5, epsilon=eps, max_iter=500)\n",
    "    epsresults.loc[eps] = [test_mse1, test_mse2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsresults"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsresults.iloc[:, 0:2].plot(kind='line', logx=True, marker='.');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example of using sigmoid utility mapping on exponential implementation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = 10 # less regularization hurts the sensitivity (and therefore max iters as well)\n",
    "tol = 0#1e-8\n",
    "max_iter = 5500\n",
    "epss = [0.01, 0.05, 0.1, 0.5, 1, 10, 100, None]\n",
    "epsresults = []\n",
    "eps_similarities = []\n",
    "eps_r2 = []\n",
    "model = \"fw-lasso-exp\"\n",
    "\n",
    "for eps in epss:\n",
    "    test_mse, feat_dict, r2, similarity = train.train(feat, best_feats, model, normalize=False, clip_sd=None, tol=tol, l=l, epsilon=eps, max_iter=max_iter, plot=inv_dir / 'out' / f'{model}_{eps}_convergence.png', triv=10.12)\n",
    "    eps_similarities.append(similarity)\n",
    "    eps_r2.append(r2)\n",
    "    epsresults.append(test_mse)\n",
    "\n",
    "rmses = np.sqrt(np.array(epsresults))\n",
    "max_rmse = 1.1*np.max(np.append(rmses, np.array([trivial_mse**.5, baseline_mse**.5]))) # buffer by 10% since models having 0 utility cannot be judged by rmse. \n",
    "c = 5 # higher values punish rmse more\n",
    "utility = 2 / (1 + np.exp(c * rmses / max_rmse))\n",
    "epsresults"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps_r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps_similarities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## main plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = 10 # less regularization hurts the sensitivity (and therefore max iters as well)\n",
    "tol = 1e-7\n",
    "max_iter = 2500\n",
    "epss = [0.01, 0.05, 0.1, 0.5, 1, 10, 100]\n",
    "epsresults = []\n",
    "eps_similarities = []\n",
    "eps_r2 = []\n",
    "model = \"compare-fw-plot\"\n",
    "\n",
    "for eps in epss:\n",
    "    test_mse, feat_dict, r2, similarity = train.train(feat, best_feats, model, normalize=False, clip_sd=None, tol=tol, l=l, epsilon=eps, max_iter=max_iter, plot=inv_dir / 'out' / f'main_lasso_{eps}.png', triv=None)\n",
    "    eps_similarities.append(similarity)\n",
    "    eps_r2.append(r2)\n",
    "    epsresults.append(test_mse)\n",
    "# plt.legend(frameon=False, fontsize=\"small\")\n",
    "plt.ylim(0, 50)\n",
    "plt.savefig(f'main_lasso_{eps}.png', dpi=300, facecolor='#EEEEEE', edgecolor='#EEEEEE', pad_inches=1)\n",
    "rmses = np.sqrt(np.array(epsresults))\n",
    "max_rmse = 1.1*np.max(np.append(rmses, np.array([trivial_mse**.5, baseline_mse**.5]))) # buffer by 10% since models having 0 utility cannot be judged by rmse. \n",
    "c = 5 # higher values punish rmse more\n",
    "utility = 2 / (1 + np.exp(c * rmses / max_rmse))\n",
    "epsresults"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_mse, r2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# non-private"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls = [1e-10, 1e-5, 1e-1, 1, 1e1, 1e2, 1e5] # less regularization hurts the sensitivity (and therefore max iters as well)\n",
    "tol = 1e-8\n",
    "max_iter = 10_000\n",
    "epsresults = []\n",
    "eps_similarities = []\n",
    "eps_r2 = []\n",
    "model = \"fw-lasso\"\n",
    "\n",
    "for l in ls:\n",
    "    test_mse, feat_dict, r2, similarity = train.train(feat, best_feats, model, normalize=False, clip_sd=None, tol=tol, l=l, max_iter=max_iter, plot=inv_dir / 'out' / f'{model}_l_{l}_convergence.png')\n",
    "    eps_similarities.append(similarity)\n",
    "    eps_r2.append(r2)\n",
    "    epsresults.append(test_mse)\n",
    "\n",
    "rmses = np.sqrt(np.array(epsresults))\n",
    "max_rmse = 1.1*np.max(np.append(rmses, np.array([trivial_mse**.5, baseline_mse**.5]))) # buffer by 10% since models having 0 utility cannot be judged by rmse. \n",
    "c = 5 # higher values punish rmse more\n",
    "utility = 2 / (1 + np.exp(c * rmses / max_rmse))\n",
    "epsresults"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "normalize=False, no clipping "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = 1 # less regularization hurts the sensitivity (and therefore max iters as well)\n",
    "tol = 1e-7\n",
    "max_iter = 5_000\n",
    "epss = [None, 0.5, 10]#[0.01, 0.05, 0.1, 0.5, 1, 10, 100, None]\n",
    "epsresults = []\n",
    "eps_similarities = []\n",
    "eps_r2 = []\n",
    "model = \"fw-lasso-exp\"\n",
    "\n",
    "for eps in epss:\n",
    "    test_mse, feat_dict, r2, similarity = train.train(feat, best_feats, model, normalize=False, tol=tol, l=l, epsilon=eps, max_iter=max_iter, plot=inv_dir / 'out' / f'{model}_{eps}_convergence.png')\n",
    "    eps_similarities.append(similarity)\n",
    "    eps_r2.append(r2)\n",
    "    epsresults.append(test_mse)\n",
    "\n",
    "rmses = np.sqrt(np.array(epsresults))\n",
    "max_rmse = 1.1*np.max(np.append(rmses, np.array([trivial_mse**.5, baseline_mse**.5]))) # buffer by 10% since models having 0 utility cannot be judged by rmse. \n",
    "c = 5 # higher values punish rmse more\n",
    "utility = 2 / (1 + np.exp(c * rmses / max_rmse))\n",
    "epsresults"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# non-private results for FW\n",
    "test_mse, r2, similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(epss, eps_r2, 'darkred')\n",
    "plt.xlabel('epsilon (log scaled)')\n",
    "plt.ylabel('r2')\n",
    "plt.axhline(trivial_r2, color='gray')\n",
    "plt.axhline(baseline_r2, color='r')\n",
    "plt.xscale('log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.clf()\n",
    "plt.plot(epss, epsresults, 'g')\n",
    "plt.xlabel('epsilon (log scaled)')\n",
    "plt.ylabel('mse')\n",
    "plt.axhline(trivial_mse, color='gray')\n",
    "plt.axhline(baseline_mse, color='r')\n",
    "plt.xscale('log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.clf()\n",
    "plt.plot(epss, eps_similarities, 'b')\n",
    "plt.xlabel('epsilon (log scaled)')\n",
    "plt.ylabel('similarity')\n",
    "plt.ylim(0, 1)\n",
    "plt.axhline(trivial_sim, color='gray')\n",
    "plt.axhline(baseline_sim, color='r')\n",
    "plt.xscale('log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = 5 # higher values punish rmse more\n",
    "utility = 2 / (1 + np.exp(c * rmses[:-1] / max_rmse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(epss[:-1], utility)\n",
    "plt.xlabel('epsilon (log scaled)')\n",
    "plt.ylabel('utility (sigmoid)')\n",
    "plt.axhline(2 / (1 + np.exp(c * epsresults[-1]**.5 / max_rmse)), color='red')\n",
    "# plt.axhline(2 / (1 + np.exp(c * trivial_mse**.5 / max_rmse)), color='gray')\n",
    "# plt.axhline(2 / (1 + np.exp(c * baseline_mse**.5 / max_rmse)), color=\"red\")\n",
    "plt.ylim(0, 1)\n",
    "plt.xscale('log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_rmse = np.max(epsresults)\n",
    "plt.plot(epss[:-1], 1 - rmses[:-1] / max_rmse)\n",
    "plt.xlabel('epsilon (log scaled)')\n",
    "plt.ylabel('utility (linear)')\n",
    "plt.axhline(1 - rmses[-1] / max_rmse, color='red')\n",
    "# plt.axhline(1 - trivial_mse**.5 / max_rmse, color='gray')\n",
    "# plt.axhline(1 - baseline_mse**.5 / max_rmse, color=\"red\")\n",
    "plt.ylim(0, 1)\n",
    "plt.xscale('log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame({'epsilon': epss[:-1],\n",
    "              'task': [\"LASSO\" for i in range(len(epss)-1)],  \n",
    "            'utility': utility.tolist()}).to_csv(\"results_draft.csv\", index_label=\"Index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cse156",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
