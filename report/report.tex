%!TEX program = xelatex
% !BIB program = bibtex
\documentclass[12pt,letterpaper]{article}
\usepackage{./style/dsc180reportstyle} % import dsc180reportstyle.sty

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%% Title and Authors
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\title{Novel Techniques in Private Telemetry Analysis}

\author{Trey Scheid \\
  {\tt tscheid@ucsd.edu} \\\And
  Tyler Kurpanek \\
  {\tt tkurpane@ucsd.edu} \\\And
  Bradley Nathanson \\
  {\tt bnathanson@ucsd.edu} \\\And
  Christopher Lum \\
  {\tt cslum@ucsd.edu} \\\And
  Yu-Xiang Wang \\
  {\tt yuxiangw@ucsd.edu} \\}

\begin{document}
% INSERT TITLE
% title is defined above
\maketitle



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%% Abstract and Links
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{abstract}    
  \textcolor{LightGrey}{Is the abstract supposed to be in light gray as well? Seems difficult to read. \
  This research investigates the practical implementation of differential privacy mechanisms for telemetry data analysis, with a focus on real-world applications. We propose a comprehensive framework that employs various privacy-preserving techniques, including randomized response and the Laplace mechanism, to protect sensitive information while maintaining analytical utility. Our methodology encompasses multiple statistical tasks, from user-level rate analysis to logistic regression classification. The study utilizes AutoDP for precise privacy loss measurement and documents the inherent tradeoffs between privacy guarantees and analytical accuracy in production environments. By demonstrating the feasibility of differential privacy in telemetry analysis, we provide a roadmap for organizations seeking to enhance their privacy practices.
  }
  \begin{center}
    Website: \url{https://endurable-gatsby-6d6.notion.site/DP-Telemetry-14556404e74780818747cbe76de2e04a?pvs=4}[Notion until actual site is created] \\
    Code: \url{https://github.com/Trey-Scheid/Novel-Techniques-in-Private-Data-Analysis}
  \end{center}
\end{abstract}


% TABLE OF CONTENTS
\maketoc

\clearpage

%%%%%%%%%%%%%%%%%%%
%%%% INTRO %%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%

\section{Introduction}

The implementation of differential privacy in production environments presents significant challenges in balancing privacy guarantees with analytical utility. This research addresses these challenges by developing practical privacy-preserving mechanisms for existing telemetry analysis tasks while maintaining the usefulness of their systems. We identify comprehensive frameworks that integrate various differential privacy mechanisms, including Guassian Composition and the Laplace mechanism, to protect sensitive information in telemetry data. Our methodology encompasses multiple statistical tasks, from user-level rate analysis to logistic regression classification, and utilizes AutoDP for precise privacy loss measurement. By evaluating the tradeoffs between privacy guarantees and analytical accuracy in production settings, we provide a roadmap for organizations looking to enhance their privacy practices.

\subsection{Motivation}

Despite the growing importance of privacy-preserving data analysis, many practitioners perceive differential privacy implementation as complex and challenging \cite{needed1}. This perception stems from several factors: the mathematical complexity of privacy definitions, the need to carefully calibrate privacy parameters, and concerns about reduced utility \cite{needed2}. A survey by Smith et al. found that only 23\% of data scientists felt confident implementing differential privacy mechanisms in their workflows \cite{needed3}.
However, recent developments have significantly lowered these barriers to entry. Tools like Google's Privacy on Beam \footnote{linkneeded}, Microsoft's SmartNoise \footnote{linkneeded}, and various open-source libraries like AutoDP\footnote{linkneeded} provide accessible frameworks for implementing differential privacy. These tools abstract away much of the underlying complexity while maintaining rigorous privacy guarantees. Additionally, educational resources and practical tutorials have emerged to guide practitioners through implementation challenges \cite{that one website Trey found its in discord}.
% citations needed to be found
% 1. A recent survey on DP adoption challenges
% 2. A paper discussing barriers to DP implementation
% 3. citation for Smith's survey of data scientists (or similar)
This research builds upon these recent developments by providing a practical demonstration of differential privacy mechanisms in telemetry data analysis. By implementing privacy-preserving techniques for existing tasks, we aim to show that differential privacy can be seamlessly integrated into production systems without significant utility loss. Our work focuses on two key objectives: privatizing existing telemetry analysis tasks and evaluating the privacy-utility tradeoffs in production settings.

\subsection{Background and Literature Review}

\subsection{Differential Privacy}

Differential privacy is a framework for data privacy that gives a mathematical guarantee that the sensitivity of each individual in the dataset is preserved. The core idea is to introduce random noise to the output of algorithms so that any single individual’s data does not significantly affect the overall result. Mathematically, a mechanism is considered (,) differentially private if for all datasets D and D’ which differ by at most 1 element when $\P [ M(D)\in S ] \leq e^\epsilon \P [ M(D') \in S)] + \delta$ where $\epsilon$ and $\delta$ are privacy loss parameters. Smaller  and  imply stronger privacy guarantees. 

Differential privacy is applied to algorithms, not datasets. One common and foundational algorithm is logistic regression. Many privatized implementations of logistic regression exist, leaving data scientists with a host of convoluted choices and complex language about parameters they may not fully understand. We hope to show some examples that will help practitioners implement this model on their own datasets.

\subsection{Intel Telemetry Data}

Differential privacy methods and guarantees are attractive for many domains. Telemetry is the remote data transfer of automated system measurements. As people use technology everyday their machines track usage diagnostics which are used by hardware and software manufacturers to reduce bugs and increase efficiency. System usage information is recorded at regular intervals and usually results in massive quantities of measurements. The identifiability of the specific machine or user of an event is a concern regardless of PIID tags. Dinur Nissim \cite{dinur} and linkage attacks can be used to recover or reconstruct the original information: the source. This is a breach of privacy for a user which depending on the sensitivity of the information can be concerning. For example, personal laptops may send diagnostics to intel given that the user opts in to the program [Intel telemetry]. 

We use a secure research database shared be Intel Corporation with consent of its users to generate real results....

\subsubsection{Errors}

In our paper, we will analyze two different types of errors. The Machine Check Architecture, or MCA, will detect an error and label it as either corrected or uncorrected. A corrected error means the system can observe and correct a detected error. Correction mechanisms include single error correction, double error correction, and more. An uncorrected error is one that was detected but not corrected or there was a computation delay long enough that the MCA treated it as an interrupted computation. \cite{add a citation}

%%%%%%%%%%%%%%%%%%%
%%%%  METHODS  %%%%%%%%
%%%%%%%%%%%%%%%%%%%

\section{Methods}

\subsection{Data Preprocessing}

Add some language here about steps that were universal between all tasks if any

For each of the follow sections we will describe the task, the algorithm used, and the implementation details.

\subsubsection{Logistic Regression (DP-SGD)}

This paper\footnote{needs citation not a footnote} investigates how privacy affects different mini-batch stochastic gradient descent algorithms for logistic regression classification. It is shown that privacy affects the batch size for optimal performance.

\subsection{Correlation (via Logistic Regression Coefficient)}

This paper \footnote{needs citation} seeks to identify whether a certain variable is disproportionately present for a certain outcome. 
More specifically, it takes a close look at two variables, max temperature on a day and whether a corrected error was present on that day. 
They would take one of those two variables and train a logistic regression model with maximum likelihood estimation to predict whether an uncorrected error was present.
From the model, they use the coefficient of the variable and make a hypothesis test whether that variable is equal to zero.

For our implementation, we focused only on whether there were corrected errors on a day, and not the variable max temperature on a day.
We add privacy to the model by using DP-SGD when training the logistic regression model, where the hypothesis test is then private by means of post-processing.

\subsubsection{LASSO Regression (DP-FW)}

will add lots of detail about lasso, then talk about adapting franke-wolfe to be differentially private. 

\subsubsection{K-Means (DP-Lloyd's)}
K-Means clustering (Lloyd's Algorithm) is applied to group devices based on similarities in their usage patterns. The method leverages Z-scores for standardizing the usage data and calculates L1 distances between weekly usage patterns to identify trends over time. Lloyd's Algorithm clusters devices by assigning them to centroids based on their usage patterns, recalculating the centroids as the mean of assigned points after each iteration. 

Differentially Private Lloyd's Algorithm (DP-Lloyd's)\footnote{linkneeded} modifies the standard K-Means clustering by adding Laplacian noise during the iterative centroid update step to ensure privacy. It introduces noise to both the sum of coordinates and the count of points within clusters, with the amount of noise controlled by the number of iterations and the sensitivity of the data. 

\subsubsection{Z-score (Additive Noise)}
As Z-score is computed before performing K-means clustering, 
\[
Z = \frac{X - \mu}{\sigma}
\]
One can privatize this clustering task by simply adding Laplacian noise to the Z-scores, though the privacy gurantee and performance between the two methods, DP-Lloyd's and Additive Noise are likely to be different.
\[
Z_{\text{private}} = \frac{X - \mu}{\sigma} + \text{Lap}\left(\frac{\Delta f}{\epsilon}\right)
\]
\begin{equation*}
\begin{aligned}
    \Delta f & \text{ is the global sensitivity of the Z-score computation,} \\
    \epsilon & \text{ is the privacy parameter.}
\end{aligned}
\end{equation*}



\subsubsection{Probabilty (Additive Noise)}
The probability of one or more uncorrected errors occurring on a system given the number of corrected errors it experienced during a specific time period is given as 
\[
P(Uncorrected|Corrected) = \frac{P(Corrected \cap Uncorrected)}{P(Corrected)}
\]

To privatize this mechanism, one can apply two methods, adding noise to the numerator and denominator, as well as clipping. The noise is sampled from a Laplace distribution with scale parameter $
\frac{\nabla f}{\epsilon}$, where $\nabla f$ is the sensitivity and $\nabla f$ is the privacy paramater. The sensitivity in this case for both the numerator and denominator will be equal to 1 because the max change in probability is equal to 1. 

In addition to the noise that is added we will regularize the data by adding a constant, $\lambda$. This constant will also be added to both the numerator and denominator so that the denominator does not get too close to 0. 
Clipping the dataset involves capping the values of data points to a predefined range, preventing extreme values from disproportionately influencing analysis or model training. We will make sure that each GUID is limited in the amount of datapoints that it can contribute. 

\subsection{Tyler task}

\subsection{Bradley task}


%%%%%%%%%%%%%%%%%%%
%%%% RESULTS  %%%%%%%%%
%%%%%%%%%%%%%%%%%%%

\section{Results}

Should we do a results section for each task separately again?
Should we do a results section for each task separately again?


\subsection{Combined Results}

We have discussed with Yu-Xiang a plot we can create which combines all the tasks into 1. 


%%%%%%%%%%%%%%%%%%%
%%%% DISCUSSION  %%%%%%%
%%%%%%%%%%%%%%%%%%%

\section{Discussion}


\subsection{Interpretation}


\subsection{Limitations}



%%%%%%%%%%%%%%%%%%%
%%%% CONCLUSION %%%%%%%
%%%%%%%%%%%%%%%%%%%

\section{Conclusion}


\subsection{Summary}


\subsection{Impact}


\subsection{Future Direction}


%%%%%%%%%%%%%%%%%%%%%%
%%%% CONTRIBUTIONS %%%
%%%%%%%%%%%%%%%%%%%%%%

\section{Contributions}

\subsection{Author Contributions}:
T.S. focused on task22 LASSO Regression to highlight the exploratory capabilities of private data while implementing a previously theoretical framework (Franke-Wolfe). C.L. implemented the algorithms in ... B.N. analyzed the experimental results ... T.K. analyzed the experimental results ... Y.W. supervised the research and provided guidance on the mathematical foundations. All authors contributed to writing and reviewing the manuscript.

\subsection{Task Details}

Trey Scheid
\begin{itemize}
    \item Replication of 
    \item Implementation of non-private franke-wolfe lasso regression
    \item Ethics considerations webpage
    \item [ ] Todo: Implementation of private franke-wolfe lasso regression
\end{itemize}

Tyler Kurpanek

Bradley Nathanson

Christopher Lum

Yu-Xiang Wang
\begin{itemize}
  \item Concept ideation
  \item Data Access
  \item Provided guidance on the mathematical foundations
  \item Proofing and editing all content
\end{itemize}


\subsection{Acknowledgements}

We would like to recognize the support of our instructor, Yu-Xiang Wang, for his guidance and feedback throughout the project. We would also like to thank the teaching staff Umesh Bellur and Shriniwas Kulkarni for their support and feedback. The tasks database was a foundational part of our work and was created by another student researcher: Qiyu Li. 

We also would like to thank the authors of the papers we referenced in our literature review. Their work was instrumental in our understanding of the topic and the development of our project. Our understandings of differential privacy has been built on the work of many researchers in the field such as: \_\_, \_\_, \_\_, and \_\_. Especially those which engaged in discussion with us about the field (Smith, Ulman, Guatam et al.). We are grateful for their contributions.



% COMMENT THIS BEFORE RENDERING
%\input{example_tex_section_6.tex}
% COMMENT THIS BEFORE RENDERING

%%%%%%%%%%%%%%%%%%%
%%%% REFERENCES%%%%%%%
%%%%%%%%%%%%%%%%%%%
\makereference

\bibliographystyle{style/dsc180bibstyle}


\bibliography{reference}
% To edit the contents of the ``References" section, edit \texttt{reference.bib}. Many conference websites format citations in BibTeX that you can copy into \texttt{reference.bib} directly; you can also search for the paper on Google Scholar, click ``Cite", and then click ``BibTeX" (\href{https://scholar.google.com/scholar?hl=en&as_sdt=0%2C23&q=attention+is+all+you+need&btnG=#d=gs_cit&t=1700436667623&u=%2Fscholar%3Fq%3Dinfo%3A5Gohgn6QFikJ%3Ascholar.google.com%2F%26output%3Dcite%26scirp%3D0%26hl%3Den}{here}'s an example).


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%% Appendix
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\clearpage
\makeappendix

\subsection{Project Proposal}


Need to add project\_proposal.tex to folder then uncomment here. 

% \input{project_proposal.tex}


\subsection{Appendix A: Additional Results}
example

\subsection{Appendix B: Training Details}
example
\subsection{Project Proposal}


% Comment out when fixed
\input{project_proposal.tex}
% Comment out when fixed


\subsection{Appendix A: Additional Results}
example

\subsection{Appendix B: Training Details}
example

\subsection{Appendix C: Additional Figures}
example


\end{document}
